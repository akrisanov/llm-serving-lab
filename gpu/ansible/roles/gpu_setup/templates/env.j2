# Environment variables for vLLM
# HuggingFace token for accessing gated models
HF_TOKEN="{{ huggingface_token | default('') }}"

# Model configuration
MODEL_NAME="{{ model_name | default('mistralai/Mistral-7B-Instruct-v0.3') }}"
VLLM_PORT={{ vllm_port | default('8000') }}
TENSOR_PARALLEL_SIZE={{ tensor_parallel_size | default('1') }}
MAX_MODEL_LEN={{ max_model_len | default('2048') }}

# API Authentication
VLLM_API_KEY="{{ vllm_api_key | default('') }}"

# Load testing configuration
VLLM_HOST="{{ ansible_default_ipv4.address | default('localhost') }}"
MAX_TOKENS="{{ max_tokens | default('128') }}"

# OpenTelemetry configuration
OTEL_EXPORTER_OTLP_ENDPOINT="http://{{ obs_otlp_endpoint | default('localhost:4317') }}"
OBS_OTLP_ENDPOINT="{{ obs_otlp_endpoint | default('localhost:4317') }}"
OTEL_SERVICE_NAME="vllm-gpu"
OTEL_RESOURCE_ATTRIBUTES="service.name=vllm-gpu,service.version=1.0.0"

# Cache directory for models
HF_HOME="/opt/vllm/models/.cache"
